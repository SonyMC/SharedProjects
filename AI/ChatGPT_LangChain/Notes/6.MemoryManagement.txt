Interactions with Memory Management
-----------------------------------

AppOverview_ChatBot.png
LLM_1.png

Completion Style LLM model:
LLM_2.png

    Refer: 
    https://platform.openai.com/playground/prompts?models=gpt-4o
    - On RHS -> Select dropdown (Default 'Responses API') -> Chat Completions API

    - ON prompt: 
        - I am a Solution Architect who jokes about designs . Have you ever noticed how designs are
            - submit 
        - AI wil ltry to complete as much a spossible


Conversation Style LLM model:
LLM_3.png


Conversation Style vs Completion Style LLM:
LLM_4.png


Conversational Style LLM Labels:
LLM_5.png


LLM_6.png


LLM_7.png

LLM_8.png

LLM_9.png

PromptTemplate.png

ChatPromptTemplate_1.png
ChatPromptTemplate_2.png
ChatPromptTemplate_3.png

ChatMemory_1.png
ChatMemory_2.png
ChatMemory_3.png
ChatMemory_4.png
ChatMemory_5.png
ChatMemory_6.png
ChatMemory_7.png
ChatMemory_8.png
ChatMemory_9.png
ChatMemory_10.png
ChatMemory_11.png
ChatMemory_12.png
ChatMemory_13.png
ChatMemory_14.png
ChatMemory_15.png
ChatMemory_16.png







Project:


chatbot
\Project\chatbot


.env 
Pipfile
main.py




    - cmd : cd \Project\chatbot_v2
    
    - cmd: pipenv shell
    - cmd: pipenv install

    - cmd: python main.py


(5) Open Source:

Note: The following project uses llama3 local model which needs to be downloaded to your local system ( ~ 5 GB)

chatbot_OpenSource
\Project\chatbot_OpenSource

.env 
Pipfile
main.py


- Install Ollama locally:
    - https://mylemans.online/posts/ollama_run_deepseek_locally/
    - https://ollama.com/download


- Start Ollama Server 
    - Download the llama3 model for Ollama

        - VS code terminal : 
            - cmd: cd n\Project
            - cmd: ollama pull llama3


- Run:
- VS code terminal : 
    - cmd: cd \Project\chatbot_OpenSource
     - cmd: pipenv install -U langchain-ollama